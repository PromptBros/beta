# beta
aim - create curated set of prompts from best online collections. Benchmark prompts and comparison across LLMs and publish "best of collections" while documenting progress. 

Plan
1. Identify sources
   
 Define specific categories and create list

 2. scrape
    use BeautifulSoup to manually collect prompts

3. store prompts in JSON or CSV

4. Define benhmarking metrics

   identify metrics and consider model specific scores

5. set up LLMs with benchmarking - integrate with APIs

6. automate the benchmarking process - script to run prompts through different LLMs and store scores

7. compare and rank prompts
analyse data, e.g. visual comparisons etc. extra calculations if needed

8. publish pack of best of with links, use cases and benchmarking scores

Immediate steps
curate set of prompts for different use cases, - AI for education, fabric etc.
research benchmarking options. 
